{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "CSE428_Fall21_Assignment_3_20101118.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "94ce5628"
      },
      "source": [
        "NAME = \"Niloy Farhan\"\n",
        "ID = \"20101118\"\n",
        "COLLABORATORS_ID = [\"21241056\", \"\"]"
      ],
      "id": "94ce5628",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4da31a6c"
      },
      "source": [
        "---"
      ],
      "id": "4da31a6c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "a60b8c4e677523727eab4f7759650400",
          "grade": false,
          "grade_id": "cell-1b4c2f2281b1520c",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "71965798"
      },
      "source": [
        "# First import the necessary libraries"
      ],
      "id": "71965798"
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "33c6128d397e1926a8f3351c7656b9b0",
          "grade": false,
          "grade_id": "cell-5543fa2e984f6297",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "3fba5b99"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import datasets, preprocessing, linear_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "id": "3fba5b99",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "d02ab800c390be8b40007a251b9f702a",
          "grade": false,
          "grade_id": "cell-3ffbb314eccd1229",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "e29cf1c8"
      },
      "source": [
        "# Linear Regression"
      ],
      "id": "e29cf1c8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "8db452901217c784823a2642df034ffb",
          "grade": true,
          "grade_id": "cell-fc331f4fdfc84046",
          "locked": false,
          "points": 3,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "95d33412",
        "outputId": "334cea5d-b6ee-451e-d0e8-cbfe6f4cda0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Import the diabetes dataset from sklearn's datasets module, save it to the variable data\n",
        "data = datasets.load_diabetes()\n",
        "\n",
        "# Split the data into X and y. Print the shape of the two array.\n",
        "X = data['data']\n",
        "y = data['target']\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "print(\"Shape of X: {}\".format(X.shape))\n",
        "print(\"Shape of y: {}\".format(y.shape))\n",
        "### END SOLUTION"
      ],
      "id": "95d33412",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X: (442, 10)\n",
            "Shape of y: (442,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "3309ba2247f726fece6e5446722ba511",
          "grade": false,
          "grade_id": "cell-8b64aba395f6a468",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "541153db"
      },
      "source": [
        "## Question 1\n",
        "How many samples are there in this datasets? How many features are there per sample?"
      ],
      "id": "541153db"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "6ed3c16c189795980c4d65ae9204adae",
          "grade": true,
          "grade_id": "cell-a5e45583a3189462",
          "locked": false,
          "points": 1,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "20f3b151"
      },
      "source": [
        "There are 442 samples in the datasets. There are 10 features per sample."
      ],
      "id": "20f3b151"
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "0496f465bbde9295deef210da33119fd",
          "grade": true,
          "grade_id": "cell-82a20292eaad0f79",
          "locked": false,
          "points": 1,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "9f9e834b",
        "outputId": "fe4f54cd-63c5-437d-f590-c8adceb1a48c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "# Display the first 5 rows of the dataset in a pandas dataframe format\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "dataframe = pd.DataFrame(X, columns = data['feature_names'])\n",
        "dataframe.head()\n",
        "### END SOLUTION"
      ],
      "id": "9f9e834b",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>bmi</th>\n",
              "      <th>bp</th>\n",
              "      <th>s1</th>\n",
              "      <th>s2</th>\n",
              "      <th>s3</th>\n",
              "      <th>s4</th>\n",
              "      <th>s5</th>\n",
              "      <th>s6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.038076</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>0.061696</td>\n",
              "      <td>0.021872</td>\n",
              "      <td>-0.044223</td>\n",
              "      <td>-0.034821</td>\n",
              "      <td>-0.043401</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>0.019908</td>\n",
              "      <td>-0.017646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.001882</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.051474</td>\n",
              "      <td>-0.026328</td>\n",
              "      <td>-0.008449</td>\n",
              "      <td>-0.019163</td>\n",
              "      <td>0.074412</td>\n",
              "      <td>-0.039493</td>\n",
              "      <td>-0.068330</td>\n",
              "      <td>-0.092204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.085299</td>\n",
              "      <td>0.050680</td>\n",
              "      <td>0.044451</td>\n",
              "      <td>-0.005671</td>\n",
              "      <td>-0.045599</td>\n",
              "      <td>-0.034194</td>\n",
              "      <td>-0.032356</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>0.002864</td>\n",
              "      <td>-0.025930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.089063</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.011595</td>\n",
              "      <td>-0.036656</td>\n",
              "      <td>0.012191</td>\n",
              "      <td>0.024991</td>\n",
              "      <td>-0.036038</td>\n",
              "      <td>0.034309</td>\n",
              "      <td>0.022692</td>\n",
              "      <td>-0.009362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.005383</td>\n",
              "      <td>-0.044642</td>\n",
              "      <td>-0.036385</td>\n",
              "      <td>0.021872</td>\n",
              "      <td>0.003935</td>\n",
              "      <td>0.015596</td>\n",
              "      <td>0.008142</td>\n",
              "      <td>-0.002592</td>\n",
              "      <td>-0.031991</td>\n",
              "      <td>-0.046641</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        age       sex       bmi  ...        s4        s5        s6\n",
              "0  0.038076  0.050680  0.061696  ... -0.002592  0.019908 -0.017646\n",
              "1 -0.001882 -0.044642 -0.051474  ... -0.039493 -0.068330 -0.092204\n",
              "2  0.085299  0.050680  0.044451  ... -0.002592  0.002864 -0.025930\n",
              "3 -0.089063 -0.044642 -0.011595  ...  0.034309  0.022692 -0.009362\n",
              "4  0.005383 -0.044642 -0.036385  ... -0.002592 -0.031991 -0.046641\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "944c29ea60db2003f79d5623719b7de2",
          "grade": true,
          "grade_id": "cell-6439d3c648bd6354",
          "locked": false,
          "points": 1,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "ce49f841",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62581f13-3749-4e11-d404-a5f5781c0c01"
      },
      "source": [
        "# Print the maximum and the minimum of each each feature\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "print(\"Feature wise maximum\")\n",
        "print(dataframe.max())\n",
        "print(\"Feature wise minimum\")\n",
        "print(dataframe.min())\n",
        "### END SOLUTION"
      ],
      "id": "ce49f841",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature wise maximum\n",
            "age    0.110727\n",
            "sex    0.050680\n",
            "bmi    0.170555\n",
            "bp     0.132044\n",
            "s1     0.153914\n",
            "s2     0.198788\n",
            "s3     0.181179\n",
            "s4     0.185234\n",
            "s5     0.133599\n",
            "s6     0.135612\n",
            "dtype: float64\n",
            "Feature wise minimum\n",
            "age   -0.107226\n",
            "sex   -0.044642\n",
            "bmi   -0.090275\n",
            "bp    -0.112400\n",
            "s1    -0.126781\n",
            "s2    -0.115613\n",
            "s3    -0.102307\n",
            "s4    -0.076395\n",
            "s5    -0.126097\n",
            "s6    -0.137767\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "07e71d69fd7fcb2e5072d90d1e8261aa",
          "grade": false,
          "grade_id": "cell-122a7394a4fd6022",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "d09f5a9d"
      },
      "source": [
        "## Question 2\n",
        "Are the minimums and the maximums close to each other? Hence, do we need any normalization or standardization?"
      ],
      "id": "d09f5a9d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "eb4c48b5f15a378ea2383227385e59fc",
          "grade": true,
          "grade_id": "cell-39ab0e0fda0b0b7d",
          "locked": false,
          "points": 1,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "66de4799"
      },
      "source": [
        "Yes, The minimums and maximums are close to each other. Hence, we don't need any normalization."
      ],
      "id": "66de4799"
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "cfc379335ef61c931f3c5ab23814d943",
          "grade": true,
          "grade_id": "cell-f5765d21afe572e7",
          "locked": false,
          "points": 1,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "11a4efcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a77feb0-b651-46ea-99b6-78e17f337a5f"
      },
      "source": [
        "# Split the data into training and testing set, with 20% of the data in the testing set. Use random_state = 100\n",
        "# Then print the shapes of X_train and X_test to see how many samples are on each set\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size= 0.20, random_state= 100) # replace with proper code\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "print(\" Shape of X_train is {}\".format(X_train.shape))\n",
        "print(\" Shape of X_test is {}\".format(X_test.shape))\n",
        "### END SOLUTION"
      ],
      "id": "11a4efcd",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Shape of X_train is (353, 10)\n",
            " Shape of X_test is (89, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "09f8411a4c260e4d265c52b95005680e",
          "grade": true,
          "grade_id": "cell-b7d35e02e74b752c",
          "locked": false,
          "points": 3,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "c99d7980"
      },
      "source": [
        "# Fit a linear regression model on X_train, y_train\n",
        "# predict the model's output for X_train and save it to y_predict_train\n",
        "# predict the model's output for X_test and save it to y_predict_test\n",
        "\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "model = linear_model.LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_predict_train = model.predict(X_train)\n",
        "y_predict_test = model.predict(X_test)\n",
        "### END SOLUTION"
      ],
      "id": "c99d7980",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "085adc72ab0f0892f3e97a457f238fab",
          "grade": true,
          "grade_id": "cell-a0556858b4619d5a",
          "locked": false,
          "points": 2,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "40b3f28b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91590b87-0efd-4e0f-fc6e-ae5fe9a23e0d"
      },
      "source": [
        "# Calculate a suitable performance metric for the training set for this regression problem\n",
        "# Calculate a suitable performance metric for the test set for this regression problem\n",
        "# Print the values\n",
        "\n",
        "performance_train = np.mean((y_train - y_predict_train)**2)**0.5\n",
        "performance_test = np.mean((y_test - y_predict_test)**2)**0.5\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "print(\"RMSE on training data=\", performance_train)\n",
        "print(\"RMSE on testing data=\", performance_test)\n",
        "### END SOLUTION"
      ],
      "id": "40b3f28b",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE on training data= 54.11079192361949\n",
            "RMSE on testing data= 51.33052418788044\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "20003a15d25b627f4725b0a4b84294f8",
          "grade": false,
          "grade_id": "cell-b6a0b1cb5248fa1c",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "c43a5b81"
      },
      "source": [
        "## Question 3\n",
        "- Which performance metric did you use and why?\n",
        "- Given the minimum and the maximum of `y` are 25 and 346, respectively, is this a good model for the dataset? Why or why not?\n",
        "- Suggest some ways (at least one) that might improve the performance metric"
      ],
      "id": "c43a5b81"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "ed0b926b3bf1872885e69175f5fb9256",
          "grade": true,
          "grade_id": "cell-aaf468f7a8397045",
          "locked": false,
          "points": 6,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "34453ce6"
      },
      "source": [
        "I use Root Mean Squared Error to measure the RMSE because it gives us an idea about the average error between actual value and prediction value. \n",
        "\n",
        "Given the minimum and the maximum of y are 25 and 346, respectively, This is not a good model. By using RMSE we find out that our mean error of our prediction is 51 in test data. For example, if y = 25 then our predicted value can be between (25-51) = -26 to 25+51 = 76 which can be a really bad prediction flactuating 2-3 times more or less. This is why it is not a good model.\n",
        "\n",
        "To improve this model, we can try other machine learning algorithms. We can also tweek the hyper parameters. Choosing the right features carefully will also improve this model significantly.\n"
      ],
      "id": "34453ce6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "763826310f3d9155ed2d80a5ebda6790",
          "grade": false,
          "grade_id": "cell-d6c580e6e22f1e37",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "bf643627"
      },
      "source": [
        "# Classification"
      ],
      "id": "bf643627"
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "540490120144eba74a27d46eaa70ab96",
          "grade": false,
          "grade_id": "cell-a6f8ea3a12b90773",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "61c867b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "c47271c6-9f6e-4ed6-f1af-f8cb1f2bd42b"
      },
      "source": [
        "# Import the sonar dataset (contained in the sonar_data.csv file)\n",
        "df = pd.read_csv(\"sonar_data.csv\")\n",
        "\n",
        "# Split it into X and y\n",
        "X = df.drop(columns=['y']).values\n",
        "y = df['y'].values\n",
        "print(\"Shape of X -\", X.shape)\n",
        "print(\"Shape of y -\", y.shape)\n",
        "print(\"Unique values of y -\", np.unique(y))\n",
        "\n",
        "# Printing the first 5 rows\n",
        "df.head()\n"
      ],
      "id": "61c867b4",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X - (208, 60)\n",
            "Shape of y - (208,)\n",
            "Unique values of y - [0 1]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x1</th>\n",
              "      <th>x2</th>\n",
              "      <th>x3</th>\n",
              "      <th>x4</th>\n",
              "      <th>x5</th>\n",
              "      <th>x6</th>\n",
              "      <th>x7</th>\n",
              "      <th>x8</th>\n",
              "      <th>x9</th>\n",
              "      <th>x10</th>\n",
              "      <th>x11</th>\n",
              "      <th>x12</th>\n",
              "      <th>x13</th>\n",
              "      <th>x14</th>\n",
              "      <th>x15</th>\n",
              "      <th>x16</th>\n",
              "      <th>x17</th>\n",
              "      <th>x18</th>\n",
              "      <th>x19</th>\n",
              "      <th>x20</th>\n",
              "      <th>x21</th>\n",
              "      <th>x22</th>\n",
              "      <th>x23</th>\n",
              "      <th>x24</th>\n",
              "      <th>x25</th>\n",
              "      <th>x26</th>\n",
              "      <th>x27</th>\n",
              "      <th>x28</th>\n",
              "      <th>x29</th>\n",
              "      <th>x30</th>\n",
              "      <th>x31</th>\n",
              "      <th>x32</th>\n",
              "      <th>x33</th>\n",
              "      <th>x34</th>\n",
              "      <th>x35</th>\n",
              "      <th>x36</th>\n",
              "      <th>x37</th>\n",
              "      <th>x38</th>\n",
              "      <th>x39</th>\n",
              "      <th>x40</th>\n",
              "      <th>x41</th>\n",
              "      <th>x42</th>\n",
              "      <th>x43</th>\n",
              "      <th>x44</th>\n",
              "      <th>x45</th>\n",
              "      <th>x46</th>\n",
              "      <th>x47</th>\n",
              "      <th>x48</th>\n",
              "      <th>x49</th>\n",
              "      <th>x50</th>\n",
              "      <th>x51</th>\n",
              "      <th>x52</th>\n",
              "      <th>x53</th>\n",
              "      <th>x54</th>\n",
              "      <th>x55</th>\n",
              "      <th>x56</th>\n",
              "      <th>x57</th>\n",
              "      <th>x58</th>\n",
              "      <th>x59</th>\n",
              "      <th>x60</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-4.7000</td>\n",
              "      <td>2.9275</td>\n",
              "      <td>25.0272</td>\n",
              "      <td>10.4968</td>\n",
              "      <td>-3.1874</td>\n",
              "      <td>21.4930</td>\n",
              "      <td>-3.3071</td>\n",
              "      <td>4.5616</td>\n",
              "      <td>30.9744</td>\n",
              "      <td>1.8999</td>\n",
              "      <td>17.7353</td>\n",
              "      <td>17.373</td>\n",
              "      <td>32.238</td>\n",
              "      <td>33.0965</td>\n",
              "      <td>11.320</td>\n",
              "      <td>-5.454</td>\n",
              "      <td>15.5100</td>\n",
              "      <td>20.8977</td>\n",
              "      <td>15.6950</td>\n",
              "      <td>30.5534</td>\n",
              "      <td>31.1443</td>\n",
              "      <td>1.6491</td>\n",
              "      <td>36.7904</td>\n",
              "      <td>35.7650</td>\n",
              "      <td>17.3688</td>\n",
              "      <td>30.6980</td>\n",
              "      <td>1.7600</td>\n",
              "      <td>13.8880</td>\n",
              "      <td>17.8283</td>\n",
              "      <td>-1.7432</td>\n",
              "      <td>34.9149</td>\n",
              "      <td>33.8644</td>\n",
              "      <td>24.0968</td>\n",
              "      <td>44.3017</td>\n",
              "      <td>27.2685</td>\n",
              "      <td>1.7154</td>\n",
              "      <td>15.7300</td>\n",
              "      <td>27.2679</td>\n",
              "      <td>24.8632</td>\n",
              "      <td>18.2928</td>\n",
              "      <td>-7.980</td>\n",
              "      <td>9.5182</td>\n",
              "      <td>12.9325</td>\n",
              "      <td>8.1072</td>\n",
              "      <td>22.5461</td>\n",
              "      <td>25.6632</td>\n",
              "      <td>-1.4776</td>\n",
              "      <td>19.1488</td>\n",
              "      <td>20.6894</td>\n",
              "      <td>-8.2224</td>\n",
              "      <td>-15.4896</td>\n",
              "      <td>9.0405</td>\n",
              "      <td>5.1430</td>\n",
              "      <td>29.2703</td>\n",
              "      <td>20.1728</td>\n",
              "      <td>13.1336</td>\n",
              "      <td>16.1080</td>\n",
              "      <td>23.1764</td>\n",
              "      <td>8.0630</td>\n",
              "      <td>3.0800</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-4.3205</td>\n",
              "      <td>3.3075</td>\n",
              "      <td>26.0232</td>\n",
              "      <td>11.6536</td>\n",
              "      <td>-2.7523</td>\n",
              "      <td>22.2915</td>\n",
              "      <td>-2.6284</td>\n",
              "      <td>7.5696</td>\n",
              "      <td>31.3392</td>\n",
              "      <td>2.5848</td>\n",
              "      <td>23.3606</td>\n",
              "      <td>24.828</td>\n",
              "      <td>36.919</td>\n",
              "      <td>45.2549</td>\n",
              "      <td>24.928</td>\n",
              "      <td>8.888</td>\n",
              "      <td>30.0000</td>\n",
              "      <td>34.4102</td>\n",
              "      <td>23.0600</td>\n",
              "      <td>37.1996</td>\n",
              "      <td>29.9452</td>\n",
              "      <td>-0.4908</td>\n",
              "      <td>36.1226</td>\n",
              "      <td>32.0022</td>\n",
              "      <td>14.6000</td>\n",
              "      <td>26.8400</td>\n",
              "      <td>-7.8225</td>\n",
              "      <td>8.0437</td>\n",
              "      <td>14.7499</td>\n",
              "      <td>-6.1328</td>\n",
              "      <td>36.6516</td>\n",
              "      <td>34.2417</td>\n",
              "      <td>21.5872</td>\n",
              "      <td>38.5751</td>\n",
              "      <td>23.6530</td>\n",
              "      <td>-7.7996</td>\n",
              "      <td>8.5875</td>\n",
              "      <td>23.7399</td>\n",
              "      <td>17.4160</td>\n",
              "      <td>17.3640</td>\n",
              "      <td>-5.652</td>\n",
              "      <td>4.3409</td>\n",
              "      <td>9.9421</td>\n",
              "      <td>4.9536</td>\n",
              "      <td>18.3041</td>\n",
              "      <td>24.2436</td>\n",
              "      <td>-2.7280</td>\n",
              "      <td>18.1872</td>\n",
              "      <td>20.7362</td>\n",
              "      <td>-8.8536</td>\n",
              "      <td>-15.7250</td>\n",
              "      <td>9.1260</td>\n",
              "      <td>5.1958</td>\n",
              "      <td>29.0816</td>\n",
              "      <td>20.2256</td>\n",
              "      <td>13.1528</td>\n",
              "      <td>16.0840</td>\n",
              "      <td>23.1029</td>\n",
              "      <td>8.0364</td>\n",
              "      <td>3.1100</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-4.6070</td>\n",
              "      <td>3.4550</td>\n",
              "      <td>26.6376</td>\n",
              "      <td>12.5992</td>\n",
              "      <td>-3.1494</td>\n",
              "      <td>22.1400</td>\n",
              "      <td>-2.3259</td>\n",
              "      <td>8.0336</td>\n",
              "      <td>34.9568</td>\n",
              "      <td>5.5746</td>\n",
              "      <td>25.7661</td>\n",
              "      <td>25.590</td>\n",
              "      <td>35.544</td>\n",
              "      <td>41.0440</td>\n",
              "      <td>22.958</td>\n",
              "      <td>3.862</td>\n",
              "      <td>23.1939</td>\n",
              "      <td>31.3673</td>\n",
              "      <td>25.3225</td>\n",
              "      <td>38.9618</td>\n",
              "      <td>35.7454</td>\n",
              "      <td>5.1477</td>\n",
              "      <td>36.7274</td>\n",
              "      <td>31.3904</td>\n",
              "      <td>16.2648</td>\n",
              "      <td>25.8956</td>\n",
              "      <td>-3.3250</td>\n",
              "      <td>14.3863</td>\n",
              "      <td>16.8468</td>\n",
              "      <td>9.4336</td>\n",
              "      <td>39.9584</td>\n",
              "      <td>36.5495</td>\n",
              "      <td>21.4896</td>\n",
              "      <td>38.9799</td>\n",
              "      <td>25.1160</td>\n",
              "      <td>-10.3054</td>\n",
              "      <td>14.2900</td>\n",
              "      <td>27.7292</td>\n",
              "      <td>25.9000</td>\n",
              "      <td>20.6628</td>\n",
              "      <td>0.294</td>\n",
              "      <td>8.9501</td>\n",
              "      <td>11.4709</td>\n",
              "      <td>5.6664</td>\n",
              "      <td>21.4331</td>\n",
              "      <td>24.2112</td>\n",
              "      <td>-0.7648</td>\n",
              "      <td>18.1904</td>\n",
              "      <td>20.2340</td>\n",
              "      <td>-8.7456</td>\n",
              "      <td>-15.9274</td>\n",
              "      <td>9.3480</td>\n",
              "      <td>5.3652</td>\n",
              "      <td>29.1615</td>\n",
              "      <td>20.4320</td>\n",
              "      <td>13.1952</td>\n",
              "      <td>16.1896</td>\n",
              "      <td>23.3444</td>\n",
              "      <td>8.0665</td>\n",
              "      <td>3.1950</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-4.8500</td>\n",
              "      <td>2.4275</td>\n",
              "      <td>25.4952</td>\n",
              "      <td>10.4920</td>\n",
              "      <td>-4.6105</td>\n",
              "      <td>21.1840</td>\n",
              "      <td>-3.7922</td>\n",
              "      <td>4.0416</td>\n",
              "      <td>26.9568</td>\n",
              "      <td>1.1376</td>\n",
              "      <td>16.4977</td>\n",
              "      <td>17.988</td>\n",
              "      <td>30.184</td>\n",
              "      <td>35.8437</td>\n",
              "      <td>13.458</td>\n",
              "      <td>-5.738</td>\n",
              "      <td>10.4553</td>\n",
              "      <td>19.2463</td>\n",
              "      <td>13.1500</td>\n",
              "      <td>28.7406</td>\n",
              "      <td>24.7561</td>\n",
              "      <td>-1.2510</td>\n",
              "      <td>39.0008</td>\n",
              "      <td>34.1458</td>\n",
              "      <td>14.5120</td>\n",
              "      <td>29.4008</td>\n",
              "      <td>-2.8600</td>\n",
              "      <td>7.7720</td>\n",
              "      <td>11.7170</td>\n",
              "      <td>-2.4584</td>\n",
              "      <td>38.3820</td>\n",
              "      <td>39.0740</td>\n",
              "      <td>24.8960</td>\n",
              "      <td>39.8467</td>\n",
              "      <td>24.9765</td>\n",
              "      <td>-10.3736</td>\n",
              "      <td>12.5200</td>\n",
              "      <td>29.1698</td>\n",
              "      <td>36.6568</td>\n",
              "      <td>26.0004</td>\n",
              "      <td>3.242</td>\n",
              "      <td>14.5138</td>\n",
              "      <td>13.7410</td>\n",
              "      <td>6.8424</td>\n",
              "      <td>26.0195</td>\n",
              "      <td>28.3848</td>\n",
              "      <td>2.3720</td>\n",
              "      <td>19.5216</td>\n",
              "      <td>21.2258</td>\n",
              "      <td>-8.2944</td>\n",
              "      <td>-15.4698</td>\n",
              "      <td>9.1815</td>\n",
              "      <td>5.0792</td>\n",
              "      <td>29.2550</td>\n",
              "      <td>20.2040</td>\n",
              "      <td>13.0584</td>\n",
              "      <td>16.0300</td>\n",
              "      <td>23.0924</td>\n",
              "      <td>8.0280</td>\n",
              "      <td>3.2925</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-3.8570</td>\n",
              "      <td>3.6650</td>\n",
              "      <td>25.1544</td>\n",
              "      <td>10.9456</td>\n",
              "      <td>-3.8790</td>\n",
              "      <td>21.3245</td>\n",
              "      <td>-3.6701</td>\n",
              "      <td>5.9472</td>\n",
              "      <td>31.7024</td>\n",
              "      <td>4.0131</td>\n",
              "      <td>22.0584</td>\n",
              "      <td>20.928</td>\n",
              "      <td>34.256</td>\n",
              "      <td>39.0295</td>\n",
              "      <td>19.056</td>\n",
              "      <td>0.652</td>\n",
              "      <td>24.3426</td>\n",
              "      <td>28.2439</td>\n",
              "      <td>8.0800</td>\n",
              "      <td>30.1992</td>\n",
              "      <td>27.7108</td>\n",
              "      <td>0.0132</td>\n",
              "      <td>39.3140</td>\n",
              "      <td>35.4177</td>\n",
              "      <td>14.5288</td>\n",
              "      <td>25.7420</td>\n",
              "      <td>1.4875</td>\n",
              "      <td>16.0000</td>\n",
              "      <td>18.4406</td>\n",
              "      <td>0.3376</td>\n",
              "      <td>37.5721</td>\n",
              "      <td>37.0049</td>\n",
              "      <td>22.3048</td>\n",
              "      <td>37.0791</td>\n",
              "      <td>23.9755</td>\n",
              "      <td>-7.8018</td>\n",
              "      <td>10.5100</td>\n",
              "      <td>25.2519</td>\n",
              "      <td>19.7872</td>\n",
              "      <td>17.9160</td>\n",
              "      <td>-5.042</td>\n",
              "      <td>8.6212</td>\n",
              "      <td>10.8787</td>\n",
              "      <td>4.0092</td>\n",
              "      <td>18.4532</td>\n",
              "      <td>24.6336</td>\n",
              "      <td>-3.1432</td>\n",
              "      <td>17.1360</td>\n",
              "      <td>20.4140</td>\n",
              "      <td>-8.8896</td>\n",
              "      <td>-15.6568</td>\n",
              "      <td>9.0465</td>\n",
              "      <td>5.1188</td>\n",
              "      <td>29.1785</td>\n",
              "      <td>20.2640</td>\n",
              "      <td>13.0120</td>\n",
              "      <td>16.0432</td>\n",
              "      <td>23.1008</td>\n",
              "      <td>8.0749</td>\n",
              "      <td>3.2350</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       x1      x2       x3       x4  ...      x58     x59     x60  y\n",
              "0 -4.7000  2.9275  25.0272  10.4968  ...  23.1764  8.0630  3.0800  0\n",
              "1 -4.3205  3.3075  26.0232  11.6536  ...  23.1029  8.0364  3.1100  0\n",
              "2 -4.6070  3.4550  26.6376  12.5992  ...  23.3444  8.0665  3.1950  0\n",
              "3 -4.8500  2.4275  25.4952  10.4920  ...  23.0924  8.0280  3.2925  0\n",
              "4 -3.8570  3.6650  25.1544  10.9456  ...  23.1008  8.0749  3.2350  0\n",
              "\n",
              "[5 rows x 61 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "f0c23ab1c6704d697c0fb167ac905cc7",
          "grade": false,
          "grade_id": "cell-31cd472e9596e497",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "83e8879d"
      },
      "source": [
        "## Question 4\n",
        "Why do we need normalization/standardization in this case?"
      ],
      "id": "83e8879d"
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the maximum and the minimum of each each feature\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "print(\"Feature wise maximum\")\n",
        "print(df.max())\n",
        "print(\"Feature wise minimum\")\n",
        "print(df.min())\n",
        "### END SOLUTION"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uldJ1emknUu6",
        "outputId": "76a3144e-c56b-465d-eafd-834d7c9aca4d"
      },
      "id": "uldJ1emknUu6",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature wise maximum\n",
            "x1     -2.9435\n",
            "x2      7.8475\n",
            "x3     31.3416\n",
            "x4     20.2336\n",
            "x5      2.6190\n",
            "        ...   \n",
            "x57    16.2130\n",
            "x58    23.9240\n",
            "x59     8.2548\n",
            "x60     4.0975\n",
            "y       1.0000\n",
            "Length: 61, dtype: float64\n",
            "Feature wise minimum\n",
            "x1     -4.9775\n",
            "x2      2.0150\n",
            "x3     24.0360\n",
            "x4     10.1392\n",
            "x5     -4.8727\n",
            "        ...   \n",
            "x57    16.0018\n",
            "x58    23.0063\n",
            "x59     8.0007\n",
            "x60     3.0150\n",
            "y       0.0000\n",
            "Length: 61, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "ce6078071530406aa7c2dbc1bce815dc",
          "grade": true,
          "grade_id": "cell-a599631efc396bd4",
          "locked": false,
          "points": 2,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "fcd61c19"
      },
      "source": [
        "As we can see, the min and max of this dataset are quite close, but not as close as our previous dataset. So, We will normalize the dataset using preprocessing.minmax_scale() function."
      ],
      "id": "fcd61c19"
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "33e948bd10f5c9fac17b5991e10022d3",
          "grade": true,
          "grade_id": "cell-f60b8a34feb6bcdb",
          "locked": false,
          "points": 2,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "89c187d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "de754f59-c381-42fa-b8ce-b022ff7d7edf"
      },
      "source": [
        "# Use min-max scaling to normalize the features of X. Save the normalized array as X_norm. \n",
        "X_norm = preprocessing.minmax_scale(X) # REPLACE WITH APPROPRIATE CODE\n",
        "\n",
        "# To confirm that it is now normalized, use the follwing code\n",
        "df_norm = pd.DataFrame(X_norm, columns=df.columns[:-1])\n",
        "df_norm.head()\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "\n",
        "### END SOLUTION"
      ],
      "id": "89c187d5",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x1</th>\n",
              "      <th>x2</th>\n",
              "      <th>x3</th>\n",
              "      <th>x4</th>\n",
              "      <th>x5</th>\n",
              "      <th>x6</th>\n",
              "      <th>x7</th>\n",
              "      <th>x8</th>\n",
              "      <th>x9</th>\n",
              "      <th>x10</th>\n",
              "      <th>x11</th>\n",
              "      <th>x12</th>\n",
              "      <th>x13</th>\n",
              "      <th>x14</th>\n",
              "      <th>x15</th>\n",
              "      <th>x16</th>\n",
              "      <th>x17</th>\n",
              "      <th>x18</th>\n",
              "      <th>x19</th>\n",
              "      <th>x20</th>\n",
              "      <th>x21</th>\n",
              "      <th>x22</th>\n",
              "      <th>x23</th>\n",
              "      <th>x24</th>\n",
              "      <th>x25</th>\n",
              "      <th>x26</th>\n",
              "      <th>x27</th>\n",
              "      <th>x28</th>\n",
              "      <th>x29</th>\n",
              "      <th>x30</th>\n",
              "      <th>x31</th>\n",
              "      <th>x32</th>\n",
              "      <th>x33</th>\n",
              "      <th>x34</th>\n",
              "      <th>x35</th>\n",
              "      <th>x36</th>\n",
              "      <th>x37</th>\n",
              "      <th>x38</th>\n",
              "      <th>x39</th>\n",
              "      <th>x40</th>\n",
              "      <th>x41</th>\n",
              "      <th>x42</th>\n",
              "      <th>x43</th>\n",
              "      <th>x44</th>\n",
              "      <th>x45</th>\n",
              "      <th>x46</th>\n",
              "      <th>x47</th>\n",
              "      <th>x48</th>\n",
              "      <th>x49</th>\n",
              "      <th>x50</th>\n",
              "      <th>x51</th>\n",
              "      <th>x52</th>\n",
              "      <th>x53</th>\n",
              "      <th>x54</th>\n",
              "      <th>x55</th>\n",
              "      <th>x56</th>\n",
              "      <th>x57</th>\n",
              "      <th>x58</th>\n",
              "      <th>x59</th>\n",
              "      <th>x60</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.136431</td>\n",
              "      <td>0.156451</td>\n",
              "      <td>0.135677</td>\n",
              "      <td>0.035426</td>\n",
              "      <td>0.224956</td>\n",
              "      <td>0.237571</td>\n",
              "      <td>0.407468</td>\n",
              "      <td>0.340904</td>\n",
              "      <td>0.449282</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.187154</td>\n",
              "      <td>0.197245</td>\n",
              "      <td>0.295667</td>\n",
              "      <td>0.038362</td>\n",
              "      <td>0.063096</td>\n",
              "      <td>0.214838</td>\n",
              "      <td>0.285048</td>\n",
              "      <td>0.272623</td>\n",
              "      <td>0.482222</td>\n",
              "      <td>0.443172</td>\n",
              "      <td>0.555544</td>\n",
              "      <td>0.496064</td>\n",
              "      <td>0.398962</td>\n",
              "      <td>0.544104</td>\n",
              "      <td>0.663012</td>\n",
              "      <td>0.605133</td>\n",
              "      <td>0.695766</td>\n",
              "      <td>0.802388</td>\n",
              "      <td>0.674412</td>\n",
              "      <td>0.345584</td>\n",
              "      <td>0.089918</td>\n",
              "      <td>0.247135</td>\n",
              "      <td>0.487661</td>\n",
              "      <td>0.777424</td>\n",
              "      <td>0.850363</td>\n",
              "      <td>0.849496</td>\n",
              "      <td>0.693309</td>\n",
              "      <td>0.594156</td>\n",
              "      <td>0.481973</td>\n",
              "      <td>0.286166</td>\n",
              "      <td>0.017371</td>\n",
              "      <td>0.339194</td>\n",
              "      <td>0.365317</td>\n",
              "      <td>0.548312</td>\n",
              "      <td>0.375462</td>\n",
              "      <td>0.190071</td>\n",
              "      <td>0.190330</td>\n",
              "      <td>0.402216</td>\n",
              "      <td>0.193337</td>\n",
              "      <td>0.392727</td>\n",
              "      <td>0.231076</td>\n",
              "      <td>0.027104</td>\n",
              "      <td>0.155844</td>\n",
              "      <td>0.435673</td>\n",
              "      <td>0.149660</td>\n",
              "      <td>0.417949</td>\n",
              "      <td>0.502841</td>\n",
              "      <td>0.185355</td>\n",
              "      <td>0.245179</td>\n",
              "      <td>0.060046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.323009</td>\n",
              "      <td>0.221603</td>\n",
              "      <td>0.272011</td>\n",
              "      <td>0.150024</td>\n",
              "      <td>0.283033</td>\n",
              "      <td>0.666756</td>\n",
              "      <td>0.574405</td>\n",
              "      <td>0.755458</td>\n",
              "      <td>0.483045</td>\n",
              "      <td>0.394537</td>\n",
              "      <td>0.656316</td>\n",
              "      <td>0.925557</td>\n",
              "      <td>0.969483</td>\n",
              "      <td>0.775910</td>\n",
              "      <td>0.745611</td>\n",
              "      <td>0.944637</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.883013</td>\n",
              "      <td>0.792131</td>\n",
              "      <td>0.766481</td>\n",
              "      <td>0.495363</td>\n",
              "      <td>0.391882</td>\n",
              "      <td>0.359648</td>\n",
              "      <td>0.376498</td>\n",
              "      <td>0.308402</td>\n",
              "      <td>0.251019</td>\n",
              "      <td>0.293098</td>\n",
              "      <td>0.255558</td>\n",
              "      <td>0.434152</td>\n",
              "      <td>0.150740</td>\n",
              "      <td>0.360327</td>\n",
              "      <td>0.285666</td>\n",
              "      <td>0.158248</td>\n",
              "      <td>0.225649</td>\n",
              "      <td>0.110770</td>\n",
              "      <td>0.413508</td>\n",
              "      <td>0.380932</td>\n",
              "      <td>0.070084</td>\n",
              "      <td>0.154860</td>\n",
              "      <td>0.201852</td>\n",
              "      <td>0.152171</td>\n",
              "      <td>0.064347</td>\n",
              "      <td>0.181172</td>\n",
              "      <td>0.209740</td>\n",
              "      <td>0.088285</td>\n",
              "      <td>0.027839</td>\n",
              "      <td>0.095980</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.206461</td>\n",
              "      <td>0.073939</td>\n",
              "      <td>0.124502</td>\n",
              "      <td>0.108417</td>\n",
              "      <td>0.218182</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.199546</td>\n",
              "      <td>0.479487</td>\n",
              "      <td>0.389205</td>\n",
              "      <td>0.105263</td>\n",
              "      <td>0.140496</td>\n",
              "      <td>0.087760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.182153</td>\n",
              "      <td>0.246892</td>\n",
              "      <td>0.356110</td>\n",
              "      <td>0.243699</td>\n",
              "      <td>0.230028</td>\n",
              "      <td>0.585327</td>\n",
              "      <td>0.648810</td>\n",
              "      <td>0.819405</td>\n",
              "      <td>0.817859</td>\n",
              "      <td>0.869584</td>\n",
              "      <td>0.856940</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.771556</td>\n",
              "      <td>0.520470</td>\n",
              "      <td>0.646805</td>\n",
              "      <td>0.688887</td>\n",
              "      <td>0.664180</td>\n",
              "      <td>0.745558</td>\n",
              "      <td>0.887334</td>\n",
              "      <td>0.852205</td>\n",
              "      <td>0.786467</td>\n",
              "      <td>0.666394</td>\n",
              "      <td>0.395253</td>\n",
              "      <td>0.349247</td>\n",
              "      <td>0.521619</td>\n",
              "      <td>0.164335</td>\n",
              "      <td>0.482088</td>\n",
              "      <td>0.849012</td>\n",
              "      <td>0.597808</td>\n",
              "      <td>0.841696</td>\n",
              "      <td>0.875204</td>\n",
              "      <td>0.521344</td>\n",
              "      <td>0.145437</td>\n",
              "      <td>0.264653</td>\n",
              "      <td>0.410044</td>\n",
              "      <td>0.298690</td>\n",
              "      <td>0.630330</td>\n",
              "      <td>0.662681</td>\n",
              "      <td>0.527514</td>\n",
              "      <td>0.501307</td>\n",
              "      <td>0.496468</td>\n",
              "      <td>0.309035</td>\n",
              "      <td>0.275314</td>\n",
              "      <td>0.286266</td>\n",
              "      <td>0.300114</td>\n",
              "      <td>0.024136</td>\n",
              "      <td>0.244114</td>\n",
              "      <td>0.222821</td>\n",
              "      <td>0.065623</td>\n",
              "      <td>0.128485</td>\n",
              "      <td>0.032869</td>\n",
              "      <td>0.319544</td>\n",
              "      <td>0.418182</td>\n",
              "      <td>0.248538</td>\n",
              "      <td>0.394558</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.889205</td>\n",
              "      <td>0.368421</td>\n",
              "      <td>0.258953</td>\n",
              "      <td>0.166282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.062684</td>\n",
              "      <td>0.070724</td>\n",
              "      <td>0.199737</td>\n",
              "      <td>0.034950</td>\n",
              "      <td>0.034999</td>\n",
              "      <td>0.071486</td>\n",
              "      <td>0.288149</td>\n",
              "      <td>0.269239</td>\n",
              "      <td>0.077447</td>\n",
              "      <td>0.164593</td>\n",
              "      <td>0.083936</td>\n",
              "      <td>0.257327</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.205012</td>\n",
              "      <td>0.170328</td>\n",
              "      <td>0.200387</td>\n",
              "      <td>0.035644</td>\n",
              "      <td>0.198026</td>\n",
              "      <td>0.375131</td>\n",
              "      <td>0.354987</td>\n",
              "      <td>0.234928</td>\n",
              "      <td>0.354872</td>\n",
              "      <td>0.529088</td>\n",
              "      <td>0.471980</td>\n",
              "      <td>0.297131</td>\n",
              "      <td>0.486067</td>\n",
              "      <td>0.501628</td>\n",
              "      <td>0.230136</td>\n",
              "      <td>0.197443</td>\n",
              "      <td>0.313838</td>\n",
              "      <td>0.629755</td>\n",
              "      <td>0.779151</td>\n",
              "      <td>0.592565</td>\n",
              "      <td>0.348172</td>\n",
              "      <td>0.381508</td>\n",
              "      <td>0.295565</td>\n",
              "      <td>0.552919</td>\n",
              "      <td>0.876677</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.985839</td>\n",
              "      <td>0.667169</td>\n",
              "      <td>0.604396</td>\n",
              "      <td>0.415104</td>\n",
              "      <td>0.412523</td>\n",
              "      <td>0.610606</td>\n",
              "      <td>0.501097</td>\n",
              "      <td>0.480804</td>\n",
              "      <td>0.471998</td>\n",
              "      <td>0.343766</td>\n",
              "      <td>0.356364</td>\n",
              "      <td>0.240040</td>\n",
              "      <td>0.161198</td>\n",
              "      <td>0.080519</td>\n",
              "      <td>0.409357</td>\n",
              "      <td>0.179138</td>\n",
              "      <td>0.176923</td>\n",
              "      <td>0.133523</td>\n",
              "      <td>0.093822</td>\n",
              "      <td>0.107438</td>\n",
              "      <td>0.256351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.550885</td>\n",
              "      <td>0.282898</td>\n",
              "      <td>0.153088</td>\n",
              "      <td>0.079886</td>\n",
              "      <td>0.132640</td>\n",
              "      <td>0.147003</td>\n",
              "      <td>0.318182</td>\n",
              "      <td>0.531863</td>\n",
              "      <td>0.516659</td>\n",
              "      <td>0.621479</td>\n",
              "      <td>0.547710</td>\n",
              "      <td>0.544549</td>\n",
              "      <td>0.586152</td>\n",
              "      <td>0.398268</td>\n",
              "      <td>0.451098</td>\n",
              "      <td>0.525544</td>\n",
              "      <td>0.720858</td>\n",
              "      <td>0.604468</td>\n",
              "      <td>0.161793</td>\n",
              "      <td>0.425942</td>\n",
              "      <td>0.383221</td>\n",
              "      <td>0.416420</td>\n",
              "      <td>0.547526</td>\n",
              "      <td>0.528634</td>\n",
              "      <td>0.299283</td>\n",
              "      <td>0.150237</td>\n",
              "      <td>0.684316</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.722200</td>\n",
              "      <td>0.437946</td>\n",
              "      <td>0.503651</td>\n",
              "      <td>0.567850</td>\n",
              "      <td>0.252441</td>\n",
              "      <td>0.081505</td>\n",
              "      <td>0.176741</td>\n",
              "      <td>0.413407</td>\n",
              "      <td>0.465012</td>\n",
              "      <td>0.294686</td>\n",
              "      <td>0.259013</td>\n",
              "      <td>0.251961</td>\n",
              "      <td>0.187493</td>\n",
              "      <td>0.291575</td>\n",
              "      <td>0.238847</td>\n",
              "      <td>0.108348</td>\n",
              "      <td>0.098379</td>\n",
              "      <td>0.072408</td>\n",
              "      <td>0.064650</td>\n",
              "      <td>0.025457</td>\n",
              "      <td>0.116103</td>\n",
              "      <td>0.055758</td>\n",
              "      <td>0.155378</td>\n",
              "      <td>0.032810</td>\n",
              "      <td>0.127273</td>\n",
              "      <td>0.277778</td>\n",
              "      <td>0.235828</td>\n",
              "      <td>0.028205</td>\n",
              "      <td>0.196023</td>\n",
              "      <td>0.102975</td>\n",
              "      <td>0.292011</td>\n",
              "      <td>0.203233</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         x1        x2        x3  ...       x58       x59       x60\n",
              "0  0.136431  0.156451  0.135677  ...  0.185355  0.245179  0.060046\n",
              "1  0.323009  0.221603  0.272011  ...  0.105263  0.140496  0.087760\n",
              "2  0.182153  0.246892  0.356110  ...  0.368421  0.258953  0.166282\n",
              "3  0.062684  0.070724  0.199737  ...  0.093822  0.107438  0.256351\n",
              "4  0.550885  0.282898  0.153088  ...  0.102975  0.292011  0.203233\n",
              "\n",
              "[5 rows x 60 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "90b1a36f7368a5c60dfe9bbf45243fe8",
          "grade": true,
          "grade_id": "cell-306bb7f1b779b785",
          "locked": false,
          "points": 2,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "919d4e97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff6724ec-dcc2-466d-fa56-58b780c4ae97"
      },
      "source": [
        "# Split the dataset (X_norm, y) into 85% training - 15% testing\n",
        "# Then split the (X_train, y_train) again into 85% training and 15% validation.\n",
        "# In both cases, use random_state=100\n",
        "# Finally, print the shape of the X_train, X_valid, and X_test\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_norm, y, test_size = 0.15, random_state= 100) # replace with proper code\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size= 0.15, random_state= 100) # replace with proper code\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "print(\"Shape of X_train ={}\".format(X_train.shape))\n",
        "print(\"Shape of X_valid ={}\".format(X_valid.shape))\n",
        "print(\"Shape of X_test ={}\".format(X_test.shape))\n",
        "### END SOLUTION"
      ],
      "id": "919d4e97",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train =(149, 60)\n",
            "Shape of X_valid =(27, 60)\n",
            "Shape of X_test =(32, 60)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "1eacd66acc8a4928f706fcc93fb3f865",
          "grade": false,
          "grade_id": "cell-017446ba40c241cc",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "7417d60c"
      },
      "source": [
        "## Quesion 5\n",
        "We want to fit a Logistic Regression model. However, instead of manually setting the value of C (the inverse of the regularization strength), we want find the optimum value.\n",
        "- On which data (train/test/valid) should we calculate the performance metric to find the optimum value of C?"
      ],
      "id": "7417d60c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "dd3224215a344382855fba7eaf967817",
          "grade": true,
          "grade_id": "cell-86701d810e16c970",
          "locked": false,
          "points": 2,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "746add93"
      },
      "source": [
        "To find the optimum value of C, we should calculate the performance state on validation data."
      ],
      "id": "746add93"
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e2de805a04a01bed65e469eedcecc444",
          "grade": true,
          "grade_id": "cell-33b508ab0187ed79",
          "locked": false,
          "points": 5,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "e43edf05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eab2e85f-f1df-4734-945c-61c1ba738ef4"
      },
      "source": [
        "# Complete the follwing code to find the optimum value of C\n",
        "C_candidate = [0.01, 0.1, 1, 10]\n",
        "performance_metric_all = {}\n",
        "\n",
        "for C in C_candidate:\n",
        "    # define the Logistic Regression model with C = C and random_state=100\n",
        "    # Fit the model using X_traina and y_train\n",
        "    # predict the model's output for the ____ dataset (which one should it be?)\n",
        "    # calculate the performance metric for the ____ dataset\n",
        "    # save it in the array performance_metric_all by using performance_metric_all[C] = per...\n",
        "    logistic_regression = linear_model.LogisticRegression(C = C, random_state= 100)\n",
        "    logistic_regression.fit( X_train, y_train)\n",
        "    y_pred_valid = logistic_regression.predict(X_valid)\n",
        "    performance_metric_all[C] = accuracy_score(y_valid,y_pred_valid)\n",
        "    pass\n",
        "\n",
        "print(performance_metric_all)\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "\n",
        "### END SOLUTION"
      ],
      "id": "e43edf05",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0.01: 0.6296296296296297, 0.1: 0.7037037037037037, 1: 0.6666666666666666, 10: 0.6666666666666666}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "e20eac89f7133612f695f345422a219d",
          "grade": false,
          "grade_id": "cell-b0351aa30e3d8419",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "f311eb02"
      },
      "source": [
        "## Quesion 6\n",
        "Based on the output of the previous block, which value of C should we choose?"
      ],
      "id": "f311eb02"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "bd209c8ba9df0c7c12abdb5a3272ba9d",
          "grade": true,
          "grade_id": "cell-e136f88268cc26d2",
          "locked": false,
          "points": 1,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "45fcbfd8"
      },
      "source": [
        "We should choose C = 0.1 because it has the highest accuracy.\n"
      ],
      "id": "45fcbfd8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "cdc6f8b214d935c2dc6720e20568480b",
          "grade": true,
          "grade_id": "cell-5b14833953d397d3",
          "locked": false,
          "points": 3,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "200c903b"
      },
      "source": [
        "# Fit a logistic regression model on X_train, y_train using the C value found above. Use random_state=100\n",
        "# predict the model's output for X_train and save it to y_predict_train\n",
        "# predict the model's output for X_test and save it to y_predict_test\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "logistic_regression = linear_model.LogisticRegression(C = 0.1, random_state = 100)\n",
        "logistic_regression.fit(X_train, y_train)\n",
        "y_pred_train =logistic_regression.predict(X_train)\n",
        "y_pred_test = logistic_regression.predict(X_test)\n",
        "### END SOLUTION"
      ],
      "id": "200c903b",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "9d147d8c9651041c8bb66823910f49e9",
          "grade": true,
          "grade_id": "cell-28abda9ee6a4686f",
          "locked": false,
          "points": 2,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "04b2242c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c244c62f-05b6-472c-f6c8-e8a77d4372f7"
      },
      "source": [
        "# Calculate a suitable performance metric for the training set for this classification problem\n",
        "# Calculate a suitable performance metric for the test set for this classification problem\n",
        "# Print the values\n",
        "\n",
        "performance_train = accuracy_score(y_train, y_pred_train)\n",
        "performance_test = accuracy_score(y_test, y_pred_test)\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "print(\"The accuracy of train set: {}\".format(performance_train))\n",
        "print(\"The accuracy of test set: {}\".format(performance_test))\n",
        "### END SOLUTION"
      ],
      "id": "04b2242c",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy of train set: 0.7986577181208053\n",
            "The accuracy of test set: 0.875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "67d658b59402862aab973c3cea5424fc",
          "grade": false,
          "grade_id": "cell-9eccac3af46884bf",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "86f3e622"
      },
      "source": [
        "## Question 7\n",
        "- Which performance metric did you use and why?\n",
        "- Is this a good model for the dataset? Why or why not?\n",
        "- Suggest some ways (at least one) that might improve the performance metric"
      ],
      "id": "86f3e622"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "161eab883c47e09d0b44eadd1c283662",
          "grade": true,
          "grade_id": "cell-87d1c00ace8c1462",
          "locked": false,
          "points": 2,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "38521349"
      },
      "source": [
        "I used Sklearns accuracy score to determine the performance of the model. I used it because, it was a classification model and I want to know how much accurate our classifications were in training and test part of our datasets.\n",
        "\n",
        "This is a quite good model for the dataset. Because it has 87.5% accuracy. But it can be improved to a better model.\n",
        "\n",
        "We can improve the performance matric in a lot of way. Some of them are given below,\n",
        "\n",
        "1. We can increase the amount of features.\n",
        "2. We can use different Machine Learning or Deep Learning algorithms.\n",
        "3. We can tweak the hyper parameters to get a better hyper parameter.\n"
      ],
      "id": "38521349"
    }
  ]
}